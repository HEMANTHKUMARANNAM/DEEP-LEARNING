{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images at...2024-08-12 12:18:25.657441\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 116\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading images at...\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mnow()))\n\u001b[0;32m    115\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m--> 116\u001b[0m \u001b[43mreadTrainData\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ImageNameDataHash)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m images at...\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mnow()))\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Reading CSV\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 55\u001b[0m, in \u001b[0;36mreadTrainData\u001b[1;34m(trainDir)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadTrainData\u001b[39m(trainDir):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m ImageNameDataHash\n\u001b[1;32m---> 55\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainDir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of files in \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m trainDir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(images)))\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m imageFileName \u001b[38;5;129;01min\u001b[39;00m images:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: ''"
     ]
    }
   ],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib\n",
    "from subprocess import check_output\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Label conversion functions\n",
    "def classes_to_int(label):\n",
    "    label = label.strip()\n",
    "    if label == \"No DR\": return 0\n",
    "    if label == \"Mild\": return 1\n",
    "    if label == \"Moderate\": return 2\n",
    "    if label == \"Severe\": return 3\n",
    "    if label == \"Proliferative DR\": return 4\n",
    "    print(\"Invalid Label\", label)\n",
    "    return 5\n",
    "\n",
    "def int_to_classes(i):\n",
    "    if i == 0: return \"No DR\"\n",
    "    elif i == 1: return \"Mild\"\n",
    "    elif i == 2: return \"Moderate\"\n",
    "    elif i == 3: return \"Severe\"\n",
    "    elif i == 4: return \"Proliferative DR\"\n",
    "    print(\"Invalid class \", i)\n",
    "    return \"Invalid Class\"\n",
    "\n",
    "# Constants\n",
    "NUM_CLASSES = 5\n",
    "WIDTH = 128\n",
    "HEIGHT = 128\n",
    "DEPTH = 3\n",
    "inputShape = (HEIGHT, WIDTH, DEPTH)\n",
    "EPOCHS = 15\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "\n",
    "# Global variables\n",
    "ImageNameDataHash = {}\n",
    "uniquePatientIDList = []\n",
    "\n",
    "# Function to read and process training data\n",
    "def readTrainData(trainDir):\n",
    "    global ImageNameDataHash\n",
    "    images = os.listdir(trainDir)\n",
    "    print(\"Number of files in \" + trainDir + \" is \" + str(len(images)))\n",
    "    for imageFileName in images:\n",
    "        if imageFileName == \"trainLabels.csv\":\n",
    "            continue\n",
    "        imageFullPath = os.path.join(os.path.sep, trainDir, imageFileName)\n",
    "        img = load_img(imageFullPath)\n",
    "        arr = img_to_array(img)  # Numpy array with shape (233,233,3)\n",
    "        dim1 = arr.shape[0]\n",
    "        dim2 = arr.shape[1]\n",
    "        dim3 = arr.shape[2]\n",
    "        if dim1 < HEIGHT or dim2 < WIDTH or dim3 < DEPTH:\n",
    "            print(\"Error: Image dimensions are less than expected \" + str(arr.shape))\n",
    "        arr = cv2.resize(arr, (HEIGHT, WIDTH))  # Resize to (128,128,3)\n",
    "        dim1 = arr.shape[0]\n",
    "        dim2 = arr.shape[1]\n",
    "        dim3 = arr.shape[2]\n",
    "        if dim1 != HEIGHT or dim2 != WIDTH or dim3 != DEPTH:\n",
    "            print(\"Error: After resize, image dimensions are not as expected \" + str(arr.shape))\n",
    "        arr = np.array(arr, dtype=\"float\") / 255.0\n",
    "        imageFileName = imageFileName.replace('.jpeg', '')\n",
    "        ImageNameDataHash[str(imageFileName)] = np.array(arr)\n",
    "    return\n",
    "\n",
    "# Function to read CSV data\n",
    "def readTrainCsv():\n",
    "    raw_df = pd.read_csv('trainLabels.csv', sep=',')\n",
    "    print(type(raw_df))  # <class 'pandas.core.frame.DataFrame'>\n",
    "    row_count = raw_df.shape[0]  # gives number of row count\n",
    "    col_count = raw_df.shape[1]  # gives number of col count\n",
    "    print(\"row_count=\" + str(row_count) + \" col_count=\" + str(col_count))\n",
    "    raw_df[\"PatientID\"] = ''\n",
    "    header_list = list(raw_df.columns)\n",
    "    print(header_list)  # ['image', 'level', 'PatientID']\n",
    "    \n",
    "    ImageLevelHash = {}\n",
    "    patientIDList = []\n",
    "    for index, row in raw_df.iterrows():\n",
    "        key = row[0] + ''\n",
    "        patientID = row[0] + ''\n",
    "        patientID = patientID.replace('_right', '')\n",
    "        patientID = patientID.replace('_left', '')\n",
    "        raw_df.at[index, 'PatientID'] = patientID\n",
    "        patientIDList.append(patientID)\n",
    "        ImageLevelHash[key] = str(row[1])  # level\n",
    "    \n",
    "    global uniquePatientIDList\n",
    "    uniquePatientIDList = sorted(set(patientIDList))\n",
    "    count = 0\n",
    "    for patientID in uniquePatientIDList:\n",
    "        left_level = ImageLevelHash[str(patientID + '_left')]\n",
    "        right_level = ImageLevelHash[str(patientID + '_right')]\n",
    "        if left_level != right_level:\n",
    "            count += 1\n",
    "    print(\"Count of images with both left and right eye level not matching=\" + str(count))\n",
    "    print(\"Number of unique patients=\" + str(len(uniquePatientIDList)))\n",
    "    return raw_df\n",
    "\n",
    "# Loading images\n",
    "print(\"Loading images at...\" + str(datetime.now()))\n",
    "sys.stdout.flush()\n",
    "readTrainData(\"\")\n",
    "print(\"Loaded \" + str(len(ImageNameDataHash)) + \" images at...\" + str(datetime.now()))\n",
    "\n",
    "# Reading CSV\n",
    "random.seed(10)\n",
    "print(\"Reading trainLabels.csv...\")\n",
    "df = readTrainCsv()\n",
    "for i in range(0, 10):\n",
    "    s = df.loc[df.index[i], 'PatientID']  # get patient id of patients\n",
    "    print(str(i) + \" patient's patientID=\" + str(s))\n",
    "\n",
    "# Filtering dataframe\n",
    "keepImages = list(ImageNameDataHash.keys())\n",
    "df = df[df['image'].isin(keepImages)]\n",
    "print(len(df))  # 1000\n",
    "\n",
    "# Convert hash to dataframe\n",
    "imageNameArr = []\n",
    "dataArr = []\n",
    "for index, row in df.iterrows():\n",
    "    key = str(row[0])\n",
    "    if key in ImageNameDataHash:\n",
    "        imageNameArr.append(key)\n",
    "        dataArr.append(np.array(ImageNameDataHash[key]))  # np.array\n",
    "df2 = pd.DataFrame({'image': imageNameArr, 'data': dataArr})\n",
    "df2_header_list = list(df2.columns)\n",
    "print(df2_header_list)  # ['image', 'data']\n",
    "print(len(df2))\n",
    "\n",
    "if len(df) != len(df2):\n",
    "    print(\"Error: Length of df != df2\")\n",
    "\n",
    "for idx in range(0, len(df)):\n",
    "    if df.loc[df.index[idx], 'image'] != df2.loc[df2.index[idx], 'image']:\n",
    "        print(\"Error \" + df.loc[df.index[idx], 'image'] + \" == \" + df2.loc[df2.index[idx], 'image'])\n",
    "\n",
    "print(df2.dtypes)\n",
    "print(df.dtypes)\n",
    "df = pd.merge(df2, df, left_on='image', right_on='image', how='outer')\n",
    "df_header_list = list(df.columns)\n",
    "print(df_header_list)  # ['image', 'data', 'level', 'PatientID']\n",
    "print(len(df))  # 1000\n",
    "print(df.sample())\n",
    "\n",
    "# Display sample image\n",
    "sample0 = df.loc[df.index[0], 'data']\n",
    "print(sample0)\n",
    "print(type(sample0))  # <class 'numpy.ndarray'>\n",
    "print(sample0.shape)  # (128, 128, 3)\n",
    "plt.imshow(sample0, interpolation='nearest')\n",
    "plt.show()\n",
    "print(\"Sample Image\")\n",
    "\n",
    "# Prepare data for training\n",
    "X = df['data']\n",
    "Y = df['level']\n",
    "Y = np.array(Y)\n",
    "Y = to_categorical(Y, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Partition the data into training and testing splits\n",
    "print(\"Partitioning data into 75:25...\")\n",
    "sys.stdout.flush()\n",
    "print(\"Unique patients in dataframe df=\" + str(df.PatientID.nunique()))  # 500\n",
    "unique_ids = df.PatientID.unique()\n",
    "print('unique_ids shape=' + str(len(unique_ids)))  # 500\n",
    "\n",
    "train_ids, valid_ids = train_test_split(unique_ids, test_size=0.25, random_state=10)\n",
    "trainid_list = train_ids.tolist()\n",
    "print('trainid_list shape=', str(len(trainid_list)))  # 375\n",
    "\n",
    "traindf = df[df.PatientID.isin(trainid_list)]\n",
    "valSet = df[~df.PatientID.isin(trainid_list)]\n",
    "traindf = traindf.reset_index(drop=True)\n",
    "valSet = valSet.reset_index(drop=True)\n",
    "trainX = traindf['data']\n",
    "trainY = traindf['level']\n",
    "valX = valSet['data']\n",
    "valY = valSet['level']\n",
    "\n",
    "print('trainX shape=', trainX.shape[0], 'valX shape=', valX.shape[0])  # 750, 250\n",
    "trainY = to_categorical(trainY, num_classes=NUM_CLASSES)\n",
    "valY = to_categorical(valY, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Construct the image generator for data augmentation\n",
    "print(\"Generating images...\")\n",
    "sys.stdout.flush()\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
    "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "# Function to create CNN model\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=inputShape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    \n",
    "    opt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Initialize the model\n",
    "model = createModel()\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "sys.stdout.flush()\n",
    "H = model.fit(aug.flow(np.array(list(trainX)), trainY, batch_size=BS),\n",
    "              validation_data=(np.array(list(valX)), valY),\n",
    "              steps_per_epoch=len(trainX) // BS,\n",
    "              epochs=EPOCHS, verbose=1)\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "print(\"Evaluating the model...\")\n",
    "loss, accuracy = model.evaluate(np.array(list(valX)), valY, verbose=1)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Plot training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, EPOCHS), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, EPOCHS), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, EPOCHS), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, EPOCHS), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
